/**
 * æµ‹è¯•æ–°çš„ç®€åŒ–ç‰ˆ SubhutiDebuggerï¼ˆæ–¹æ¡ˆCï¼‰
 * 
 * éªŒè¯åŠŸèƒ½ï¼š
 * - âœ… è§„åˆ™æ‰§è¡Œï¼ˆè¿›å…¥/é€€å‡ºï¼‰
 * - âœ… Token æ¶ˆè´¹ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰
 * - âœ… ç¼“å­˜å‘½ä¸­æ ‡è¯†
 * - âœ… è€—æ—¶ä¿¡æ¯
 * - âœ… åµŒå¥—å±‚çº§ï¼ˆç¼©è¿›ï¼‰
 * - âœ… Or åˆ†æ”¯é€‰æ‹©
 * - âœ… å›æº¯æ ‡è¯†
 */

import SubhutiLexer from './src/SubhutiLexer.ts'
import SubhutiParser, { Subhuti, SubhutiRule } from './src/SubhutiParser.ts'
import SubhutiTokenConsumer from './src/SubhutiTokenConsumer.ts'
import { createKeywordToken, createRegToken, createValueRegToken, SubhutiCreateTokenGroupType } from './src/struct/SubhutiCreateToken.ts'
import type { SubhutiTokenConsumerConstructor } from './src/SubhutiParser.ts'
import type SubhutiMatchToken from './src/struct/SubhutiMatchToken.ts'

// ============================================
// 1. å®šä¹‰ç®€å•çš„ Token
// ============================================

const tokensObj = {
    ImportTok: createKeywordToken('ImportTok', 'import'),
    FromTok: createKeywordToken('FromTok', 'from'),
    Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
    String: createRegToken('String', /"[^"]*"/),
    LBrace: createRegToken('LBrace', /\{/),
    RBrace: createRegToken('RBrace', /\}/),
    WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', SubhutiCreateTokenGroupType.skip),
}

const tokens = Object.values(tokensObj)

// ============================================
// 2. å®šä¹‰ Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
    ImportTok() { return this.consume(tokensObj.ImportTok) }
    FromTok() { return this.consume(tokensObj.FromTok) }
    Identifier() { return this.consume(tokensObj.Identifier) }
    String() { return this.consume(tokensObj.String) }
    LBrace() { return this.consume(tokensObj.LBrace) }
    RBrace() { return this.consume(tokensObj.RBrace) }
}

// ============================================
// 3. å®šä¹‰ç®€å•çš„ Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
    constructor(
        tokens?: SubhutiMatchToken[],
        TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
    ) {
        super(tokens, TokenConsumerClass)
    }
    @SubhutiRule
    ImportDeclaration(): SubhutiCst | undefined {
        this.tokenConsumer.ImportTok()
        this.ImportClause()
        this.tokenConsumer.FromTok()
        this.tokenConsumer.String()
        return this.curCst
    }
    
    @SubhutiRule
    ImportClause(): SubhutiCst | undefined {
        // Or è§„åˆ™ï¼šæµ‹è¯•åˆ†æ”¯é€‰æ‹©
        this.Or([
            // åˆ†æ”¯ 1ï¼š{ name }
            {
                alt: () => {
                    this.tokenConsumer.LBrace()
                    this.tokenConsumer.Identifier()
                    this.tokenConsumer.RBrace()
                }
            },
            // åˆ†æ”¯ 2ï¼šnameï¼ˆç®€å•å½¢å¼ï¼‰
            {
                alt: () => {
                    this.tokenConsumer.Identifier()
                }
            }
        ])
        return this.curCst
    }
}

// ============================================
// 4. æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('ğŸ§ª æµ‹è¯•æ–°çš„ç®€åŒ–ç‰ˆ SubhutiDebugger\n')
console.log('â•'.repeat(80))

// æµ‹è¯• 1ï¼šæˆåŠŸçš„è§£æï¼ˆOr åˆ†æ”¯ 1 æˆåŠŸï¼‰
console.log('\nğŸ“‹ æµ‹è¯• 1ï¼šOr åˆ†æ”¯ 1 æˆåŠŸ')
console.log('â”€'.repeat(80))
const code1 = 'import { lodash } from "lodash"'
const lexer1 = new SubhutiLexer(tokens)
const tokenStream1 = lexer1.tokenize(code1)

// è°ƒè¯•ï¼šæ‰“å° tokenStream ç»“æ„
console.log('tokenStream1:', tokenStream1)
console.log('tokenStream1.tokens:', tokenStream1.tokens)
console.log('')

// å‡è®¾ tokenStream1 å°±æ˜¯ tokens æ•°ç»„
const parser1 = new TestParser(tokenStream1 as any).debug()  // å¯ç”¨è°ƒè¯•
const cst1 = parser1.ImportDeclaration()

console.log(parser1.getDebugTrace())
console.log('\nâœ… è§£æç»“æœ:', cst1 ? 'SUCCESS' : 'FAILED')

// æµ‹è¯• 2ï¼šOr åˆ†æ”¯å›æº¯ï¼ˆåˆ†æ”¯ 1 å¤±è´¥ï¼Œåˆ†æ”¯ 2 æˆåŠŸï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 2ï¼šOr åˆ†æ”¯å›æº¯ï¼ˆåˆ†æ”¯ 1 å¤±è´¥ â†’ åˆ†æ”¯ 2 æˆåŠŸï¼‰')
console.log('â”€'.repeat(80))
const code2 = 'import lodash from "lodash"'
const lexer2 = new SubhutiLexer(tokens)
const tokenStream2 = lexer2.tokenize(code2)
const parser2 = new TestParser(tokenStream2 as any).debug()  // å¯ç”¨è°ƒè¯•
const cst2 = parser2.ImportDeclaration()

console.log(parser2.getDebugTrace())
console.log('\nâœ… è§£æç»“æœ:', cst2 ? 'SUCCESS' : 'FAILED')

// æµ‹è¯• 3ï¼šç¼“å­˜å‘½ä¸­ï¼ˆç¬¬äºŒæ¬¡è§£æåŒä¸€è§„åˆ™ï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 3ï¼šç¼“å­˜å‘½ä¸­ï¼ˆç¬¬äºŒæ¬¡è§£æï¼‰')
console.log('â”€'.repeat(80))
const lexer3 = new SubhutiLexer(tokens)
const tokenStream3 = lexer3.tokenize(code1)
const parser3 = new TestParser(tokenStream3 as any).debug()

// ç¬¬ä¸€æ¬¡è§£æ
parser3.ImportDeclaration()
parser3.debuggerInstance?.clear()  // æ¸…ç©ºè°ƒè¯•è®°å½•

// é‡ç½® parserï¼ˆæ¨¡æ‹Ÿç¬¬äºŒæ¬¡è§£æï¼‰
;(parser3 as any).tokenIndex = 0
;(parser3 as any)._parseFailed = false

// ç¬¬äºŒæ¬¡è§£æï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
parser3.ImportDeclaration()
console.log(parser3.getDebugTrace())
console.log('\nâœ… åº”è¯¥çœ‹åˆ° âš¡CACHED æ ‡è¯†')

console.log('\n\n' + 'â•'.repeat(80))
console.log('âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼')
console.log('\nğŸ“Š é¢„æœŸè¾“å‡ºæ ¼å¼ï¼š')
console.log('  â¡ï¸  è§„åˆ™å    âš¡CACHED  (è€—æ—¶ms)')
console.log('    ğŸ”¹ Consume  token[index] - value - <TokenName>  âœ…')
console.log('    ğŸ”€ Or[N branches]  trying #index  @token[index]')
console.log('    âª Backtrack  token[from] â†’ token[to]  (reason)')

