/**
 * æµ‹è¯•åˆå¹¶åçš„æ€§èƒ½ç»Ÿè®¡åŠŸèƒ½ï¼ˆv3.0ï¼‰
 * 
 * éªŒè¯ï¼š
 * - âœ… debug() åŒæ—¶æä¾›è¿‡ç¨‹è¿½è¸ªå’Œæ€§èƒ½ç»Ÿè®¡
 * - âœ… getSummary() æ€§èƒ½æ‘˜è¦
 * - âœ… getShortSummary() å•è¡Œæ‘˜è¦
 * - âœ… getStats() åŸå§‹æ•°æ®è®¿é—®
 * - âœ… å‘åå…¼å®¹ï¼šprofiling() ä»å¯ç”¨
 */

import SubhutiLexer from './src/SubhutiLexer.ts'
import SubhutiParser, { Subhuti, SubhutiRule } from './src/SubhutiParser.ts'
import SubhutiTokenConsumer from './src/SubhutiTokenConsumer.ts'
import { createKeywordToken, createRegToken, createValueRegToken, SubhutiCreateTokenGroupType } from './src/struct/SubhutiCreateToken.ts'
import type { SubhutiTokenConsumerConstructor } from './src/SubhutiParser.ts'
import type SubhutiMatchToken from './src/struct/SubhutiMatchToken.ts'

// ============================================
// å®šä¹‰ Token
// ============================================

const tokensObj = {
    IfTok: createKeywordToken('IfTok', 'if'),
    LParen: createRegToken('LParen', /\(/),
    RParen: createRegToken('RParen', /\)/),
    LBrace: createRegToken('LBrace', /\{/),
    RBrace: createRegToken('RBrace', /\}/),
    Identifier: createRegToken('Identifier', /[a-z]+/),
    WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', SubhutiCreateTokenGroupType.skip),
}

const tokens = Object.values(tokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
    IfTok() { return this.consume(tokensObj.IfTok) }
    LParen() { return this.consume(tokensObj.LParen) }
    RParen() { return this.consume(tokensObj.RParen) }
    LBrace() { return this.consume(tokensObj.LBrace) }
    RBrace() { return this.consume(tokensObj.RBrace) }
    Identifier() { return this.consume(tokensObj.Identifier) }
}

// ============================================
// Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
    constructor(
        tokens?: SubhutiMatchToken[],
        TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
    ) {
        super(tokens, TokenConsumerClass)
    }
    
    @SubhutiRule
    Program() {
        this.Statement()
        return this.curCst
    }
    
    @SubhutiRule
    Statement() {
        this.IfStatement()
        return this.curCst
    }
    
    @SubhutiRule
    IfStatement() {
        this.tokenConsumer.IfTok()
        this.tokenConsumer.LParen()
        this.Expression()
        this.tokenConsumer.RParen()
        this.Block()
        return this.curCst
    }
    
    @SubhutiRule
    Expression() {
        this.tokenConsumer.Identifier()
        return this.curCst
    }
    
    @SubhutiRule
    Block() {
        this.tokenConsumer.LBrace()
        this.tokenConsumer.Identifier()
        this.tokenConsumer.RBrace()
        return this.curCst
    }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('ğŸ§ª æµ‹è¯•åˆå¹¶åçš„æ€§èƒ½ç»Ÿè®¡åŠŸèƒ½ï¼ˆv3.0ï¼‰\n')
console.log('â•'.repeat(80))

// æµ‹è¯• 1ï¼šç¬¬ä¸€æ¬¡è§£æï¼ˆæ— ç¼“å­˜ï¼‰
console.log('\nğŸ“‹ æµ‹è¯• 1ï¼šç¬¬ä¸€æ¬¡è§£æï¼ˆå»ºç«‹ç¼“å­˜ï¼‰')
console.log('â”€'.repeat(80))
const code1 = 'if (x) { y }'
const lexer1 = new SubhutiLexer(tokens)
const tokenStream1 = lexer1.tokenize(code1)

const parser1 = new TestParser(tokenStream1 as any)
    .cache()    // å¯ç”¨ç¼“å­˜
    .debug()    // å¯ç”¨è°ƒè¯•ï¼ˆç°åœ¨åŒ…å«æ€§èƒ½ç»Ÿè®¡ï¼‰

const cst1 = parser1.Program()

console.log('è¿‡ç¨‹è¿½è¸ª:')
console.log(parser1.getDebugTrace())
console.log('')

console.log('æ€§èƒ½æ‘˜è¦:')
console.log(parser1.getProfilingReport())
console.log('')

console.log('ç®€æ´æ‘˜è¦:')
console.log(parser1.getProfilingShortReport())

// æµ‹è¯• 2ï¼šç¬¬äºŒæ¬¡è§£æï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 2ï¼šç¬¬äºŒæ¬¡è§£æï¼ˆç¼“å­˜å‘½ä¸­ï¼‰')
console.log('â”€'.repeat(80))
const lexer2 = new SubhutiLexer(tokens)
const tokenStream2 = lexer2.tokenize(code1)

const parser2 = new TestParser(tokenStream2 as any).debug()

// ç¬¬ä¸€æ¬¡è§£æï¼ˆå»ºç«‹ç¼“å­˜ï¼‰
parser2.Program()

// æ¸…ç©ºè°ƒè¯•è½¨è¿¹ï¼Œä¿ç•™ç»Ÿè®¡
parser2.debuggerInstance?.clear()

// é‡ç½® parser
parser2.setTokens(lexer2.tokenize(code1) as any)

// ç¬¬äºŒæ¬¡è§£æï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
parser2.Program()

console.log('è¿‡ç¨‹è¿½è¸ªï¼ˆåº”çœ‹åˆ°âš¡CACHEDï¼‰:')
console.log(parser2.getDebugTrace())
console.log('')

console.log('æ€§èƒ½æ‘˜è¦ï¼ˆåº”æœ‰ç¼“å­˜å‘½ä¸­ï¼‰:')
console.log(parser2.getProfilingReport())

// æµ‹è¯• 3ï¼šå‘åå…¼å®¹æ€§ï¼ˆprofiling() APIï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 3ï¼šå‘åå…¼å®¹ï¼ˆprofiling() APIï¼‰')
console.log('â”€'.repeat(80))
const lexer3 = new SubhutiLexer(tokens)
const tokenStream3 = lexer3.tokenize(code1)

const parser3 = new TestParser(tokenStream3 as any)
    .profiling()  // ä½¿ç”¨æ—§ APIï¼ˆåº”è¯¥ç­‰åŒäº debug()ï¼‰

parser3.Program()

console.log('ä½¿ç”¨ profiling() API:')
console.log(parser3.getProfilingShortReport())
console.log('âœ… å‘åå…¼å®¹æˆåŠŸ')

// æµ‹è¯• 4ï¼šåŸå§‹æ•°æ®è®¿é—®
console.log('\n\nğŸ“‹ æµ‹è¯• 4ï¼šåŸå§‹æ•°æ®è®¿é—®')
console.log('â”€'.repeat(80))
const stats = parser1.getProfilingStats()

if (stats) {
    console.log('è§„åˆ™ç»Ÿè®¡æ•°æ®:')
    for (const [ruleName, stat] of stats) {
        console.log(`  ${ruleName}:`)
        console.log(`    æ€»è°ƒç”¨: ${stat.totalCalls}`)
        console.log(`    å®é™…æ‰§è¡Œ: ${stat.actualExecutions}`)
        console.log(`    ç¼“å­˜å‘½ä¸­: ${stat.cacheHits}`)
        console.log(`    å¹³å‡è€—æ—¶: ${(stat.avgTime * 1000).toFixed(1)}Î¼s`)
    }
    console.log('âœ… åŸå§‹æ•°æ®è®¿é—®æˆåŠŸ')
}

console.log('\n\n' + 'â•'.repeat(80))
console.log('âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼')
console.log('\nğŸ“Š éªŒè¯åŠŸèƒ½ï¼š')
console.log('  1. âœ… debug() åŒæ—¶æä¾›è¿‡ç¨‹è¿½è¸ªå’Œæ€§èƒ½ç»Ÿè®¡')
console.log('  2. âœ… getSummary() æ€§èƒ½æ‘˜è¦ï¼ˆè¯¦ç»†ï¼‰')
console.log('  3. âœ… getShortSummary() å•è¡Œæ‘˜è¦')
console.log('  4. âœ… getStats() åŸå§‹æ•°æ®')
console.log('  5. âœ… profiling() å‘åå…¼å®¹')

