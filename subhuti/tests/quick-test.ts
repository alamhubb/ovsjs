/**
 * å¿«é€Ÿæµ‹è¯•æ–°çš„ SubhutiParser
 */

import SubhutiParser, { Subhuti, SubhutiRule } from '../src/parser/SubhutiParser.ts'
import SubhutiLexer from '../src/parser/SubhutiLexer.ts'
import SubhutiTokenConsumer from '../src/parser/SubhutiTokenConsumer.ts'
import { createKeywordToken, createRegToken } from '../src/struct/SubhutiCreateToken.ts'

// å®šä¹‰ç®€å•çš„ Token
const tokens = [
    createRegToken('WhiteSpace', /\s+/, true),  // è·³è¿‡ç©ºæ ¼
    createKeywordToken('IfTok', 'if'),
    createKeywordToken('ElseTok', 'else'),
    createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
    createRegToken('Number', /[0-9]+/),
    createRegToken('LParen', /\(/),
    createRegToken('RParen', /\)/),
    createRegToken('LBrace', /\{/),
    createRegToken('RBrace', /\}/),
    createRegToken('Semicolon', /;/),
    createRegToken('Plus', /\+/),
    createRegToken('Assign', /=/),
]

// å®šä¹‰ TokenConsumer
class TestTokenConsumer extends SubhutiTokenConsumer {
    IfTok() { this.instance.consume('IfTok') }
    ElseTok() { this.instance.consume('ElseTok') }
    Identifier() { return this.instance.consume('Identifier') }
    Number() { return this.instance.consume('Number') }
    LParen() { this.instance.consume('LParen') }
    RParen() { this.instance.consume('RParen') }
    LBrace() { this.instance.consume('LBrace') }
    RBrace() { this.instance.consume('RBrace') }
    Semicolon() { this.instance.consume('Semicolon') }
    Plus() { this.instance.consume('Plus') }
    Assign() { this.instance.consume('Assign') }
}

// å®šä¹‰ç®€å•çš„ Parser
@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
    constructor(tokens) {
        super(tokens, TestTokenConsumer)
    }
    
    @SubhutiRule
    Program() {
        this.Many(() => this.Statement())
    }
    
    @SubhutiRule
    Statement() {
        this.Or([
            { alt: () => this.IfStatement() },
            { alt: () => this.Assignment() }
        ])
    }
    
    @SubhutiRule
    IfStatement() {
        this.tokenConsumer.IfTok()
        this.tokenConsumer.LParen()
        this.Expression()
        this.tokenConsumer.RParen()
        this.tokenConsumer.LBrace()
        this.Statement()
        this.tokenConsumer.RBrace()
        this.Option(() => {
            this.tokenConsumer.ElseTok()
            this.tokenConsumer.LBrace()
            this.Statement()
            this.tokenConsumer.RBrace()
        })
    }
    
    @SubhutiRule
    Assignment() {
        this.tokenConsumer.Identifier()
        this.tokenConsumer.Assign()
        this.Expression()
        this.tokenConsumer.Semicolon()
    }
    
    @SubhutiRule
    Expression() {
        this.Or([
            { alt: () => {
                this.tokenConsumer.Identifier()
                this.tokenConsumer.Plus()
                this.tokenConsumer.Number()
            }},
            { alt: () => this.tokenConsumer.Identifier() },
            { alt: () => this.tokenConsumer.Number() }
        ])
    }
}

// æµ‹è¯•ç”¨ä¾‹
console.log('ğŸ§ª æµ‹è¯•æ–°çš„ SubhutiParser\n')

// æµ‹è¯•1ï¼šç®€å•èµ‹å€¼
console.log('æµ‹è¯•1ï¼šç®€å•èµ‹å€¼')
const code1 = 'x = 5;'
const lexer1 = new SubhutiLexer(tokens)
const tokenStream1 = lexer1.tokenize(code1)
const parser1 = new TestParser(tokenStream1)

try {
    const cst1 = parser1.Program()
    console.log('âœ… è§£ææˆåŠŸ')
    console.log('CST èŠ‚ç‚¹æ•°:', cst1?.children?.length || 0)
} catch (error) {
    console.log('âŒ è§£æå¤±è´¥:', error.message)
}

// æµ‹è¯•2ï¼šifè¯­å¥
console.log('\næµ‹è¯•2ï¼šifè¯­å¥')
const code2 = 'if (x) { y = 1; }'
const lexer2 = new SubhutiLexer(tokens)
const tokenStream2 = lexer2.tokenize(code2)
const parser2 = new TestParser(tokenStream2)

try {
    const cst2 = parser2.Program()
    console.log('âœ… è§£ææˆåŠŸ')
    console.log('CST èŠ‚ç‚¹æ•°:', cst2?.children?.length || 0)
} catch (error) {
    console.log('âŒ è§£æå¤±è´¥:', error.message)
}

// æµ‹è¯•3ï¼šPackrat Parsing ç»Ÿè®¡
console.log('\næµ‹è¯•3ï¼šPackrat Parsing ç»Ÿè®¡')
const code3 = 'x = 1; y = 2; z = 3;'
const lexer3 = new SubhutiLexer(tokens)
const tokenStream3 = lexer3.tokenize(code3)
const parser3 = new TestParser(tokenStream3)

try {
    const cst3 = parser3.Program()
    const stats = parser3.getMemoStats()
    console.log('âœ… è§£ææˆåŠŸ')
    console.log('ç¼“å­˜å‘½ä¸­:', stats.hits)
    console.log('ç¼“å­˜æœªå‘½ä¸­:', stats.misses)
    console.log('ç¼“å­˜å¤§å°:', stats.cacheSize)
    console.log('å‘½ä¸­ç‡:', stats.hitRate)
} catch (error) {
    console.log('âŒ è§£æå¤±è´¥:', error.message)
}

console.log('\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ')

