/**
 * æµ‹è¯•æ–°çš„ç»Ÿä¸€API
 * 
 * å±•ç¤º4ä¸ªåŠŸèƒ½å¼€å…³ï¼š
 * - cache()       - ç¼“å­˜
 * - debug()       - è°ƒè¯•
 * - profiling()   - æ€§èƒ½åˆ†æ
 * - errorHandler() - é”™è¯¯å¤„ç†
 */

import SubhutiLexer from './src/SubhutiLexer.ts'
import { createKeywordToken, createRegToken, createValueRegToken } from './src/struct/SubhutiCreateToken.ts'
import SubhutiParser, { Subhuti, SubhutiRule } from './src/SubhutiParser.ts'
import SubhutiTokenConsumer from './src/SubhutiTokenConsumer.ts'

// ============================================
// 1. å®šä¹‰ Tokens
// ============================================

const tokens = [
    createKeywordToken('ImportTok', 'import'),
    createKeywordToken('FromTok', 'from'),
    createKeywordToken('LBrace', '{'),
    createKeywordToken('RBrace', '}'),
    createKeywordToken('Comma', ','),
    createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
    createRegToken('StringLiteral', /"[^"]*"/),
    createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', 'skip'),
]

// ============================================
// 2. å®šä¹‰ Parser
// ============================================

class ImportTokenConsumer extends SubhutiTokenConsumer {
    ImportTok = () => this.instance.consume('ImportTok')
    FromTok = () => this.instance.consume('FromTok')
    LBrace = () => this.instance.consume('LBrace')
    RBrace = () => this.instance.consume('RBrace')
    Comma = () => this.instance.consume('Comma')
    Identifier = () => this.instance.consume('Identifier')
    StringLiteral = () => this.instance.consume('StringLiteral')
}

@Subhuti
class ImportParser extends SubhutiParser<ImportTokenConsumer> {
    constructor(tokens: any[]) {
        super(tokens, ImportTokenConsumer)
    }
    
    @SubhutiRule
    ImportDeclaration() {
        this.tokenConsumer.ImportTok()
        this.ImportClause()
        this.tokenConsumer.FromTok()
        this.tokenConsumer.StringLiteral()
    }
    
    @SubhutiRule
    ImportClause() {
        this.tokenConsumer.LBrace()
        this.NamedImports()
        this.tokenConsumer.RBrace()
    }
    
    @SubhutiRule
    NamedImports() {
        this.ImportSpecifier()
        this.Many(() => {
            this.tokenConsumer.Comma()
            this.ImportSpecifier()
        })
    }
    
    @SubhutiRule
    ImportSpecifier() {
        this.tokenConsumer.Identifier()
    }
}

// ============================================
// 3. æµ‹è¯•åœºæ™¯
// ============================================

console.log('='.repeat(60))
console.log('ğŸ§ª æµ‹è¯•æ–°API - ç»Ÿä¸€çš„é“¾å¼è°ƒç”¨')
console.log('='.repeat(60))
console.log()

const sourceCode = 'import { name, age, city } from "./foo"'

console.log('ğŸ“ Source Code:')
console.log(sourceCode)
console.log()

// Lexer
const lexer = new SubhutiLexer(tokens)
const tokenStream = lexer.tokenize(sourceCode)

console.log('ğŸ”¤ Tokens:')
tokenStream.forEach((token, i) => {
    console.log(`  [${i}] ${token.tokenName.padEnd(15)} "${token.tokenValue}"`)
})
console.log()

// ============================================
// åœºæ™¯1ï¼šé»˜è®¤é…ç½®ï¼ˆcacheå¼€å¯ï¼Œå…¶ä»–å…³é—­ï¼‰
// ============================================

console.log('â”'.repeat(60))
console.log('ğŸ“ åœºæ™¯1ï¼šé»˜è®¤é…ç½®')
console.log('   cache: å¼€å¯ | debug: å…³é—­ | profiling: å…³é—­ | errorHandler: å¼€å¯')
console.log('â”'.repeat(60))

const parser1 = new ImportParser(tokenStream)  // é»˜è®¤é…ç½®
const cst1 = parser1.ImportDeclaration()

console.log('âœ… è§£ææˆåŠŸ')
console.log(`   CSTèŠ‚ç‚¹: ${cst1?.name}`)
console.log()

// ============================================
// åœºæ™¯2ï¼šå¼€å‘æ¨¡å¼ï¼ˆå…¨å¼€ï¼‰
// ============================================

console.log('â”'.repeat(60))
console.log('ğŸ“ åœºæ™¯2ï¼šå¼€å‘æ¨¡å¼ï¼ˆé“¾å¼è°ƒç”¨ - å…¨å¼€ï¼‰')
console.log('   cache: å¼€å¯ | debug: å¼€å¯ | profiling: å¼€å¯ | errorHandler: å¼€å¯')
console.log('â”'.repeat(60))

const parser2 = new ImportParser(tokenStream)
    .cache()        // å¼€å¯ç¼“å­˜ï¼ˆé»˜è®¤trueï¼Œå¯çœç•¥ï¼‰
    .debug()        // å¼€å¯è°ƒè¯•
    .profiling()    // å¼€å¯æ€§èƒ½åˆ†æ

const cst2 = parser2.ImportDeclaration()

console.log('âœ… è§£ææˆåŠŸ')
console.log()

// è°ƒè¯•è½¨è¿¹
console.log('ğŸ” è°ƒè¯•è½¨è¿¹ï¼ˆå‰10è¡Œï¼‰:')
const trace = parser2.getDebugTrace() || ''
console.log(trace.split('\n').slice(0, 10).join('\n'))
console.log('   ...')
console.log()

// æ€§èƒ½æŠ¥å‘Š
console.log('â±ï¸  æ€§èƒ½æŠ¥å‘Š:')
console.log(parser2.getProfilingReport())
console.log()

// ============================================
// åœºæ™¯3ï¼šç”Ÿäº§æ¨¡å¼ï¼ˆç®€åŒ–é”™è¯¯ï¼‰
// ============================================

console.log('â”'.repeat(60))
console.log('ğŸ“ åœºæ™¯3ï¼šç”Ÿäº§æ¨¡å¼ï¼ˆç®€åŒ–é”™è¯¯ï¼‰')
console.log('   cache: å¼€å¯ | debug: å…³é—­ | profiling: å…³é—­ | errorHandler: å…³é—­')
console.log('â”'.repeat(60))

const parser3 = new ImportParser(tokenStream)
    .errorHandler(false)  // ç®€åŒ–é”™è¯¯ä¿¡æ¯

const cst3 = parser3.ImportDeclaration()

console.log('âœ… è§£ææˆåŠŸï¼ˆä½¿ç”¨ç®€åŒ–é”™è¯¯æ¨¡å¼ï¼‰')
console.log()

// ============================================
// åœºæ™¯4ï¼šæµ‹è¯•è¯¦ç»†é”™è¯¯ vs ç®€å•é”™è¯¯
// ============================================

console.log('â”'.repeat(60))
console.log('ğŸ“ åœºæ™¯4ï¼šé”™è¯¯å¯¹æ¯”ï¼ˆè¯¦ç»† vs ç®€å•ï¼‰')
console.log('â”'.repeat(60))

const badCode = 'import { name, age from "./foo"'  // ç¼ºå°‘ }
const badTokens = lexer.tokenize(badCode)

console.log('âŒ é”™è¯¯ä»£ç :', badCode)
console.log()

// è¯¦ç»†é”™è¯¯
console.log('ã€è¯¦ç»†é”™è¯¯æ¨¡å¼ã€‘')
try {
    const parser4a = new ImportParser(badTokens)
        .errorHandler()  // è¯¦ç»†é”™è¯¯ï¼ˆé»˜è®¤ï¼‰
    parser4a.ImportDeclaration()
} catch (e: any) {
    console.log(e.toString())
}
console.log()

// ç®€å•é”™è¯¯
console.log('ã€ç®€å•é”™è¯¯æ¨¡å¼ã€‘')
try {
    const parser4b = new ImportParser(badTokens)
        .errorHandler(false)  // ç®€å•é”™è¯¯
    parser4b.ImportDeclaration()
} catch (e: any) {
    console.log(e.toString())
}
console.log()

// ============================================
// åœºæ™¯5ï¼šå…³é—­ç¼“å­˜ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰
// ============================================

console.log('â”'.repeat(60))
console.log('ğŸ“ åœºæ™¯5ï¼šå…³é—­ç¼“å­˜ï¼ˆçœ‹çœŸå®æ€§èƒ½ï¼‰')
console.log('   cache: å…³é—­ | debug: å…³é—­ | profiling: å¼€å¯ | errorHandler: å¼€å¯')
console.log('â”'.repeat(60))

const parser5 = new ImportParser(tokenStream)
    .cache(false)   // å…³é—­ç¼“å­˜
    .profiling()    // å¼€å¯æ€§èƒ½åˆ†æ

const cst5 = parser5.ImportDeclaration()

console.log('âœ… è§£ææˆåŠŸï¼ˆæ— ç¼“å­˜ï¼‰')
console.log()
console.log('â±ï¸  æ€§èƒ½æŠ¥å‘Šï¼ˆæ— ç¼“å­˜ï¼‰:')
console.log(parser5.getProfilingReport())
console.log()

// ============================================
// æ€»ç»“
// ============================================

console.log('='.repeat(60))
console.log('âœ… æµ‹è¯•å®Œæˆ')
console.log('='.repeat(60))
console.log()
console.log('ğŸ“ æ–°APIç‰¹æ€§ï¼š')
console.log('   1. âœ… é“¾å¼è°ƒç”¨ - parser.cache().debug().profiling()')
console.log('   2. âœ… ç»Ÿä¸€é£æ ¼ - æ‰€æœ‰åŠŸèƒ½éƒ½æ˜¯ method(enable = true)')
console.log('   3. âœ… é»˜è®¤é…ç½® - cacheå¼€å¯ï¼Œå…¶ä»–å…³é—­')
console.log('   4. âœ… ç®€æ´æ–¹æ³• - getDebugTrace() ä»£æ›¿ debuggerInstance.getTrace()')
console.log('   5. âœ… 4ä¸ªåŠŸèƒ½ - cache / debug / profiling / errorHandler')
console.log()

