/**
 * æµ‹è¯•ç®€åŒ–åçš„é”™è¯¯å¤„ç†ç³»ç»Ÿ
 * 
 * éªŒè¯ï¼š
 * - âœ… ParsingError åŸºæœ¬åŠŸèƒ½
 * - âœ… æ™ºèƒ½å»ºè®®ç”Ÿæˆï¼ˆ5 ç§æ ¸å¿ƒåœºæ™¯ï¼‰
 * - âœ… è¯¦ç»†æ ¼å¼ vs ç®€å•æ ¼å¼
 */

import SubhutiLexer from './src/SubhutiLexer.ts'
import SubhutiParser, { Subhuti, SubhutiRule } from './src/SubhutiParser.ts'
import SubhutiTokenConsumer from './src/SubhutiTokenConsumer.ts'
import { createKeywordToken, createRegToken, createValueRegToken, SubhutiCreateTokenGroupType } from './src/struct/SubhutiCreateToken.ts'
import type { SubhutiTokenConsumerConstructor } from './src/SubhutiParser.ts'
import type SubhutiMatchToken from './src/struct/SubhutiMatchToken.ts'

// ============================================
// å®šä¹‰ Token
// ============================================

const tokensObj = {
    LetTok: createKeywordToken('LetTok', 'let'),
    Identifier: createRegToken('Identifier', /[a-zA-Z_][a-zA-Z0-9_]*/),
    Number: createRegToken('Number', /[0-9]+/),
    LBrace: createRegToken('LBrace', /\{/),
    RBrace: createRegToken('RBrace', /\}/),
    Semicolon: createRegToken('Semicolon', /;/),
    WhiteSpace: createValueRegToken('WhiteSpace', /[ \t\r\n]+/, '', SubhutiCreateTokenGroupType.skip),
}

const tokens = Object.values(tokensObj)

// ============================================
// Token Consumer
// ============================================

class TestTokenConsumer extends SubhutiTokenConsumer {
    LetTok() { return this.consume(tokensObj.LetTok) }
    Identifier() { return this.consume(tokensObj.Identifier) }
    Number() { return this.consume(tokensObj.Number) }
    LBrace() { return this.consume(tokensObj.LBrace) }
    RBrace() { return this.consume(tokensObj.RBrace) }
    Semicolon() { return this.consume(tokensObj.Semicolon) }
}

// ============================================
// Parser
// ============================================

@Subhuti
class TestParser extends SubhutiParser<TestTokenConsumer> {
    constructor(
        tokens?: SubhutiMatchToken[],
        TokenConsumerClass: SubhutiTokenConsumerConstructor<TestTokenConsumer> = TestTokenConsumer as SubhutiTokenConsumerConstructor<TestTokenConsumer>
    ) {
        super(tokens, TokenConsumerClass)
    }
    
    @SubhutiRule
    Statement() {
        this.tokenConsumer.LetTok()
        this.tokenConsumer.Identifier()
        this.tokenConsumer.LBrace()
        this.tokenConsumer.RBrace()
        this.tokenConsumer.Semicolon()
        return this.curCst
    }
}

// ============================================
// æµ‹è¯•ç”¨ä¾‹
// ============================================

console.log('ğŸ§ª æµ‹è¯•ç®€åŒ–åçš„é”™è¯¯å¤„ç†ç³»ç»Ÿ\n')
console.log('â•'.repeat(80))

// æµ‹è¯• 1ï¼šç¼ºå°‘é—­åˆèŠ±æ‹¬å·
console.log('\nğŸ“‹ æµ‹è¯• 1ï¼šç¼ºå°‘é—­åˆèŠ±æ‹¬å· }')
console.log('â”€'.repeat(80))
try {
    const code1 = 'let x { ;'  // ç¼ºå°‘ }
    const lexer1 = new SubhutiLexer(tokens)
    const tokenStream1 = lexer1.tokenize(code1)
    const parser1 = new TestParser(tokenStream1 as any)
    parser1.Statement()
} catch (error: any) {
    console.log(error.toString())
}

// æµ‹è¯• 2ï¼šç¼ºå°‘åˆ†å·
console.log('\n\nğŸ“‹ æµ‹è¯• 2ï¼šç¼ºå°‘åˆ†å· ;')
console.log('â”€'.repeat(80))
try {
    const code2 = 'let x { }'  // ç¼ºå°‘ ;
    const lexer2 = new SubhutiLexer(tokens)
    const tokenStream2 = lexer2.tokenize(code2)
    const parser2 = new TestParser(tokenStream2 as any)
    parser2.Statement()
} catch (error: any) {
    console.log(error.toString())
}

// æµ‹è¯• 3ï¼šå…³é”®å­—æ‹¼å†™é”™è¯¯
console.log('\n\nğŸ“‹ æµ‹è¯• 3ï¼šå…³é”®å­—æ‹¼å†™é”™è¯¯')
console.log('â”€'.repeat(80))
try {
    const code3 = 'lat x { } ;'  // lat ä¸æ˜¯å…³é”®å­—
    const lexer3 = new SubhutiLexer(tokens)
    const tokenStream3 = lexer3.tokenize(code3)
    const parser3 = new TestParser(tokenStream3 as any)
    parser3.Statement()
} catch (error: any) {
    console.log(error.toString())
}

// æµ‹è¯• 4ï¼šæ ‡è¯†ç¬¦é”™è¯¯ï¼ˆæ•°å­—å¼€å¤´ï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 4ï¼šæ ‡è¯†ç¬¦é”™è¯¯ï¼ˆæ•°å­—å¼€å¤´ï¼‰')
console.log('â”€'.repeat(80))
try {
    const code4 = 'let 123 { } ;'  // æ•°å­—ä¸èƒ½ä½œä¸ºæ ‡è¯†ç¬¦
    const lexer4 = new SubhutiLexer(tokens)
    const tokenStream4 = lexer4.tokenize(code4)
    const parser4 = new TestParser(tokenStream4 as any)
    parser4.Statement()
} catch (error: any) {
    console.log(error.toString())
}

// æµ‹è¯• 5ï¼šEOFï¼ˆä»£ç æ„å¤–ç»“æŸï¼‰
console.log('\n\nğŸ“‹ æµ‹è¯• 5ï¼šä»£ç æ„å¤–ç»“æŸï¼ˆEOFï¼‰')
console.log('â”€'.repeat(80))
try {
    const code5 = 'let'  // ä»£ç ä¸å®Œæ•´
    const lexer5 = new SubhutiLexer(tokens)
    const tokenStream5 = lexer5.tokenize(code5)
    const parser5 = new TestParser(tokenStream5 as any)
    parser5.Statement()
} catch (error: any) {
    console.log(error.toString())
}

// æµ‹è¯• 6ï¼šç®€å•æ ¼å¼ vs è¯¦ç»†æ ¼å¼
console.log('\n\nğŸ“‹ æµ‹è¯• 6ï¼šç®€å•æ ¼å¼ vs è¯¦ç»†æ ¼å¼')
console.log('â”€'.repeat(80))
try {
    const code6 = 'let x { ;'
    const lexer6 = new SubhutiLexer(tokens)
    const tokenStream6 = lexer6.tokenize(code6)
    const parser6 = new TestParser(tokenStream6 as any)
    
    // è®¾ç½®ç®€å•æ¨¡å¼
    ;(parser6 as any)._errorHandler.setDetailed(false)
    
    parser6.Statement()
} catch (error: any) {
    console.log('ç®€å•æ ¼å¼ï¼š')
    console.log(error.toString())
}

console.log('\n\n' + 'â•'.repeat(80))
console.log('âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆï¼')
console.log('\nğŸ“Š éªŒè¯åŠŸèƒ½ï¼š')
console.log('  1. âœ… ç¼ºå°‘é—­åˆç¬¦å·æç¤º')
console.log('  2. âœ… ç¼ºå°‘åˆ†å·æç¤º')
console.log('  3. âœ… å…³é”®å­—æ‹¼å†™é”™è¯¯æç¤º')
console.log('  4. âœ… æ ‡è¯†ç¬¦é”™è¯¯æç¤º')
console.log('  5. âœ… EOF æç¤º')
console.log('  6. âœ… ç®€å•æ ¼å¼ vs è¯¦ç»†æ ¼å¼')

